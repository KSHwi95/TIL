# **크롤링**

> 웹페이지의 모든 데이터를 가져오는 것

## **VS 스크레이핑**

- 원하는 부분만 수정해서 필요한 데이터를 추출하는것.
  > 수업에서는 크롤링과 스크레이핑을 굳이 구분하지는 않는다.

방법

### 1. API

### 2. HTML&CSS

### 3. 브라우저 컨트롤

<br>

### **4.Scrapi** - 스크레이핑 프레임워크

: 위의 세 툴이 크롤링 테스트를 위함이라면, 직접적으로 크롤러 개발을 위한 도구.

|                 | 속도 | 난이도 |
| :-------------: | :--: | :----: |
|       API       |  上  |   下   |
|    HTML css     |  中  |   中   |
| Browser Control | 下下 |   上   |

### **과정**

- API를 허용한 웹사이트에서 APP키를 부여받음

## **(0) 크롤링 요청 : `requests` 라이브러리**

방식 : GET / POST / PUT / Delete (CRUD)

## **(1) 웹 API 크롤링**

```python
# 크롤링 요청 : Requests
import requests
WEBSITE_URL='https://~.com'
# API키 받기
WEB_API_KEY = "121412v41512321s31231a"
header = {f"Authorization": "WEBAK{WEB_API_KEY}"}

header
# 웹페이지 데이터 받기
res = requests.get(WEBSITE_URL,header)

# response.json()  : 시리얼라이즈 된 json 데이터를 자동으로 디시얼리라이즈 해줌
# > JSON 데이터를 여러 개 나열한 JSONArray
data = res.json()
```

### **응답 코드(Response Code)**

- `1xx` : 처리 중 ( 볼 일이 없다 )
- **`2xx`** : Success
- `3xx` : Redirection ( 추가 동작이 필요하다. - 볼 일이 많이 없다 )
- **`4xx`** : Client Error ( 클라이언트에서 오류 발생 )
  - `404` : Page Not Found (잘못된 url로 요청함)
- **`5xx`** : Server Error ( 서버에서 오류발생 )

## **(2) HTML 스크레이핑 `BeautifulSoup`**

크롤링한 데이터를 `BeautifulSoup` 객체로서 담는다.

방식 : `<객체몀> = BeautiuflSoup(__.content, 'html.parser')`

- 의미 : 담을 대상, 전환할 방식

### **메서드**

1. find

- find("태그명", "속성 값을 딕셔너리로 표현") : 한 개의 엘리먼트 찾기
- find_all("태그명", "속성 값을 딕셔너리로 표현") : 여러 개의 엘리먼트 찾기

2. select <br>
   `선택자(`Selector`)를 사용해서 찾기`

- `select("선택자")` : 선택자에 의해 엘리먼트를 여러 개 선택
- `select_one("선택자")` : 선택자에 의해 엘리먼트를 한 개만 선택

> element를 한 개 선택했을때, `.text("")`를 통한 `텍스트 추출` 과 `['__']`을 통한 `속성 추출` 이 가능.

```python
# 크롤링 요청 : Requests
WEBSITE_URL='https://~.com'
import requests
# 방식 - 스크레이핑 : BeautifulSoup
from bs4 import BeautifulSoup
# 웹페이지 데이터 받기
res = requests.get(WEBSITE_URL)
soup = BeautifulSoup(res.content, 'html.parser')

soup.find("")
#또는
soup.select("")

```
